{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cdb13b4-6edb-4c73-8664-5fd16d26903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import docx\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb25880-8472-46c4-8f76-1817146c988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deeda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e205ed-3f01-4e70-a0e6-12356bcff6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing text files\n",
    "directory_path = r\"C:\\\\Users\\\\deeda\\\\Desktop\\\\Brandeis\\\\Second Year\\\\NLP_anlaysis\\\\transcripts\"\n",
    "\n",
    "#save csv in this folder\n",
    "save_file_path = r\"C:\\\\Users\\\\deeda\\\\Desktop\\\\Brandeis\\\\Second Year\\\\NLP_anlaysis\\\\Cardiff_chunk_results\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebf5c56-2b6e-4b9d-9800-62c4eddbf25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'@\\w+', '@user', text)\n",
    "    text = re.sub(r'http\\S+', 'http', text)\n",
    "    return text\n",
    "\n",
    "# Load .docx file\n",
    "def load_docx_text(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    return \"\\n\".join([para.text for para in doc.paragraphs if para.text.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa3f05a-ba7c-4c5b-a19e-d571dda938f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking function\n",
    "def chunk_text(text, max_tokens=100, stride=20):\n",
    "    tokens = tokenizer(preprocess(text), return_tensors='pt', truncation=False)['input_ids'][0]\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), max_tokens - stride):\n",
    "        chunk_tokens = tokens[i:i + max_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n",
    "        chunks.append(chunk_text)\n",
    "        if i + max_tokens >= len(tokens):\n",
    "            break\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "088138e7-27f2-4ad9-ab74-1f1e754e3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment prediction function\n",
    "def predict_sentiment(text_chunk):\n",
    "    inputs = tokenizer(preprocess(text_chunk), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    scores = softmax(outputs.logits[0].numpy())\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "199f8fdf-1e88-46f7-9e48-42be292b5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results\n",
    "def analyze_transcript(file_path):\n",
    "    try:\n",
    "        text = load_docx_text(file_path)\n",
    "        if not text.strip():\n",
    "            return None\n",
    "\n",
    "        chunks = chunk_text(text)\n",
    "        if not chunks:\n",
    "            return None\n",
    "\n",
    "        chunk_results = []\n",
    "        sentiment_scores = []\n",
    "\n",
    "        for chunk in chunks:\n",
    "            scores = predict_sentiment(chunk)\n",
    "            sentiment_scores.append(scores)\n",
    "            chunk_results.append({\n",
    "                \"chunk_text\": chunk,\n",
    "                \"predicted_label\": config.id2label[int(scores.argmax())],\n",
    "                \"score_negative\": scores[0],\n",
    "                \"score_neutral\": scores[1],\n",
    "                \"score_positive\": scores[2],\n",
    "            })\n",
    "\n",
    "        avg_scores = np.mean(sentiment_scores, axis=0)\n",
    "        overall_label = config.id2label[int(avg_scores.argmax())]\n",
    "        return {\n",
    "            \"filename\": os.path.basename(file_path),\n",
    "            \"overall_sentiment\": overall_label,\n",
    "            \"score_negative\": avg_scores[0],\n",
    "            \"score_neutral\": avg_scores[1],\n",
    "            \"score_positive\": avg_scores[2]\n",
    "        }, chunk_results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] {file_path}: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3be51e6-54c1-44c8-beab-ca82efed0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all files in folder\n",
    "def analyze_folder(directory_path):\n",
    "    summary_results = []\n",
    "    chunk_data = []\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".docx\"):\n",
    "            result = analyze_transcript(os.path.join(directory_path, filename))\n",
    "            if result:\n",
    "                summary, chunks = result\n",
    "                summary_results.append(summary)\n",
    "                for c in chunks:\n",
    "                    c[\"filename\"] = filename\n",
    "                    chunk_data.append(c)\n",
    "\n",
    "    return summary_results, chunk_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea595448-e19d-487f-981a-193f80338b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis\n",
    "#folder_path = \"path/to/your/folder\"\n",
    "summary_results, chunk_data = analyze_folder(directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78f507bd-5a4b-4c06-8ef3-a4be32da264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "#for filename, label, scores in summary_results:\n",
    "    #print(f\"\\nFile: {filename}\")\n",
    "    #print(f\"Overall Sentiment: {label}\")\n",
    "    #for i, label_name in config.id2label.items():\n",
    "        #print(f\"  {label_name}: {scores[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "681fffdf-1971-435b-b40c-e308ed4b6bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert results to a DataFrame\n",
    "pd.DataFrame(summary_results).to_csv(os.path.join(save_file_path, \"summary_sentiment_chunks.csv\"), index=False)\n",
    "pd.DataFrame(chunk_data).to_csv(os.path.join(save_file_path, \"chunk_sentiment.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0c3cd4-5b03-4bac-967c-20b4fae63412",
   "metadata": {},
   "source": [
    "Average sentiment scores across movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7098b6d-3600-47ad-8a66-19039bcc534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate and Average Sentiment Scores across movies\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_participant_id(filename):\n",
    "    match = re.match(r\"(.*?)_(box|piper|umbrella)\\.docx\", filename)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "def summarize_participants(summary_results):\n",
    "    participant_scores = defaultdict(list)\n",
    "    for entry in summary_results:\n",
    "        pid = extract_participant_id(entry[\"filename\"])\n",
    "        if pid:\n",
    "            participant_scores[pid].append(entry)\n",
    "\n",
    "    participant_summary = []\n",
    "    for pid, entries in participant_scores.items():\n",
    "        n = len(entries)\n",
    "        total_neg = sum(e[\"score_negative\"] for e in entries)\n",
    "        total_neu = sum(e[\"score_neutral\"] for e in entries)\n",
    "        total_pos = sum(e[\"score_positive\"] for e in entries)\n",
    "        total = total_neg + total_neu + total_pos\n",
    "        participant_summary.append({\n",
    "            \"participant_id\": pid,\n",
    "            \"avg_score_negative\": total_neg / n,\n",
    "            \"avg_score_neutral\": total_neu / n,\n",
    "            \"avg_score_positive\": total_pos / n,\n",
    "            #\"percent_negative\": total_neg / total,\n",
    "            #\"percent_neutral\": total_neu / total,\n",
    "            #\"percent_positive\": total_pos / total,\n",
    "            \"num_movies\": n\n",
    "        })\n",
    "    return participant_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6f5b5ec-3384-45c4-82b2-e782ef8aa936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all sentiment analysis results based on chunk-level.\n"
     ]
    }
   ],
   "source": [
    "# Save participant-level averages and percentages\n",
    "participant_summary = summarize_participants(summary_results)\n",
    "pd.DataFrame(participant_summary).to_csv(os.path.join(save_file_path, \"participant_avg_sentiment_chunks_just averages.csv\"), index=False)\n",
    "\n",
    "print(\"Saved all sentiment analysis results based on chunk-level.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e5fcf-64f3-436d-abd4-4c9f19fdfa2a",
   "metadata": {},
   "source": [
    "Sentiment scores per movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d84040c3-ca01-4bfc-8156-664c02487723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\\\Users\\\\deeda\\\\Desktop\\\\Brandeis\\\\Second Year\\\\NLP_anlaysis\\\\Cardiff_chunk_results\\box_participant_sentiment.csv\n",
      "Saved: C:\\\\Users\\\\deeda\\\\Desktop\\\\Brandeis\\\\Second Year\\\\NLP_anlaysis\\\\Cardiff_chunk_results\\piper_participant_sentiment.csv\n",
      "Saved: C:\\\\Users\\\\deeda\\\\Desktop\\\\Brandeis\\\\Second Year\\\\NLP_anlaysis\\\\Cardiff_chunk_results\\umbrella_participant_sentiment.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_movie_name(filename):\n",
    "    for movie in [\"box\", \"piper\", \"umbrella\"]:\n",
    "        if filename.endswith(f\"_{movie}.docx\"):\n",
    "            return movie\n",
    "    return \"other\"\n",
    "\n",
    "# Group entries by movie\n",
    "movie_groups = defaultdict(list)\n",
    "\n",
    "for entry in summary_results:\n",
    "    filename = entry[\"filename\"]\n",
    "    participant_id = extract_participant_id(filename)\n",
    "    movie = extract_movie_name(filename)\n",
    "    \n",
    "    if participant_id and movie in [\"box\", \"piper\", \"umbrella\"]:\n",
    "        movie_groups[movie].append({\n",
    "            \"participant_id\": participant_id,\n",
    "            \"filename\": filename,\n",
    "            \"overall_sentiment\": entry[\"overall_sentiment\"],\n",
    "            \"score_negative\": entry[\"score_negative\"],\n",
    "            \"score_neutral\": entry[\"score_neutral\"],\n",
    "            \"score_positive\": entry[\"score_positive\"]\n",
    "        })\n",
    "\n",
    "# Save one CSV per movie\n",
    "for movie, records in movie_groups.items():\n",
    "    df = pd.DataFrame(records)\n",
    "    csv_path = os.path.join(save_file_path, f\"{movie}_participant_sentiment.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84cfc7-91b6-44c8-83d4-3aedb9a144c9",
   "metadata": {},
   "source": [
    "Averages per Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e780cb7d-fb2b-4736-b57f-8cac4b944e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved average sentiment summary for each movie.\n"
     ]
    }
   ],
   "source": [
    "# Group by movie type\n",
    "from collections import defaultdict\n",
    "\n",
    "movie_scores = defaultdict(list)\n",
    "\n",
    "for entry in summary_results:\n",
    "    if entry[\"filename\"].endswith(\"_box.docx\"):\n",
    "        movie = \"box\"\n",
    "    elif entry[\"filename\"].endswith(\"_piper.docx\"):\n",
    "        movie = \"piper\"\n",
    "    elif entry[\"filename\"].endswith(\"_umbrella.docx\"):\n",
    "        movie = \"umbrella\"\n",
    "    else:\n",
    "        continue  # skip unmatched\n",
    "    movie_scores[movie].append(entry)\n",
    "\n",
    "# Compute average sentiment scores per movie\n",
    "movie_summary = []\n",
    "for movie, entries in movie_scores.items():\n",
    "    n = len(entries)\n",
    "    total_neg = sum(e[\"score_negative\"] for e in entries)\n",
    "    total_neu = sum(e[\"score_neutral\"] for e in entries)\n",
    "    total_pos = sum(e[\"score_positive\"] for e in entries)\n",
    "    total = total_neg + total_neu + total_pos\n",
    "\n",
    "    movie_summary.append({\n",
    "        \"movie\": movie,\n",
    "        \"avg_score_negative\": total_neg / n,\n",
    "        \"avg_score_neutral\": total_neu / n,\n",
    "        \"avg_score_positive\": total_pos / n,\n",
    "        \"percent_negative\": total_neg / total,\n",
    "        \"percent_neutral\": total_neu / total,\n",
    "        \"percent_positive\": total_pos / total,\n",
    "        \"num_participants\": n\n",
    "    })\n",
    "\n",
    "# Save to CSV\n",
    "df_movie = pd.DataFrame(movie_summary)\n",
    "df_movie.to_csv(os.path.join(save_file_path, \"movie_avg_sentiment_summary.csv\"), index=False)\n",
    "\n",
    "print(\"Saved average sentiment summary for each movie.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73a924-f1e9-486c-ae94-1432dfdfcb83",
   "metadata": {},
   "source": [
    "Particpants who have all the 3movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "139eedf5-1375-466d-b146-c0f633fb26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group scores by participant\n",
    "participant_movies = defaultdict(dict)\n",
    "\n",
    "for entry in summary_results:\n",
    "    pid = extract_participant_id(entry[\"filename\"])\n",
    "    movie = extract_movie_name(entry[\"filename\"])\n",
    "    if pid and movie:\n",
    "        participant_movies[pid][movie] = entry\n",
    "\n",
    "\n",
    "participant_summary = []\n",
    "for pid, movies in participant_movies.items():\n",
    "    if all(movie in movies for movie in [\"box\", \"piper\", \"umbrella\"]):\n",
    "        scores = [movies[movie] for movie in [\"box\", \"piper\", \"umbrella\"]]\n",
    "        avg_neg = sum(e[\"score_negative\"] for e in scores) / 3\n",
    "        avg_neu = sum(e[\"score_neutral\"] for e in scores) / 3\n",
    "        avg_pos = sum(e[\"score_positive\"] for e in scores) / 3\n",
    "        participant_summary.append({\n",
    "            \"participant_id\": pid,\n",
    "            \"avg_score_negative\": avg_neg,\n",
    "            \"avg_score_neutral\": avg_neu,\n",
    "            \"avg_score_positive\": avg_pos\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e5bc5d6-f72e-4759-922d-91b320d6a875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved participants who watched all 3 movies with averaged sentiment.\n"
     ]
    }
   ],
   "source": [
    "df_complete = pd.DataFrame(participant_summary)\n",
    "df_complete.to_csv(os.path.join(save_file_path, \"participants_all3_avg_sentiment.csv\"), index=False)\n",
    "\n",
    "print(\"Saved participants who watched all 3 movies with averaged sentiment.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
